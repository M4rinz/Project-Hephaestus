{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simpler models with the same results. Can this be considered a victory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 18:31:48.704683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735666308.818533   68460 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735666308.851408   68460 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-31 18:31:49.089376: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from utility.classification_utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building our beautiful dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%  \n"
     ]
    }
   ],
   "source": [
    "cyc = '../dataset/cyclists_cleaned.csv'\n",
    "races = '../dataset/races_cleaned.csv'\n",
    "df = make_dataset_for_classification(races, cyc, make_stage_type=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumb imputations\n",
    "df['height'] = df['height'].fillna(df['height'].mean())\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "df['bmi'] = df['weight']/np.square(df['height']/100)\n",
    "df['cyclist_age_rac'] = df['cyclist_age_rac'].fillna(df['cyclist_age_rac'].mean())\n",
    "df['climb_total'] = df['climb_total'].fillna(df['length']*0.05) # could be differentiated by lengths\n",
    "df['steepness'] = df['length'] / df['climb_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for profile, let's use the big stick\n",
    "# Features and target\n",
    "# Add a column 'is_ITT' where 1 indicates stage_type is 'ITT', 0 otherwise\n",
    "df['is_ITT'] = (df['stage_type'] == 'ITT').astype(int)\n",
    "df_filtered = df[['length', 'climb_total', 'steepness', 'average_speed', 'is_ITT', 'profile']].dropna()\n",
    "X = df_filtered[['length', 'climb_total', 'steepness', 'average_speed', 'is_ITT']]\n",
    "y = df_filtered['profile']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9991312777770783\n",
      "Feature importance: [0.21644605 0.27903885 0.30263699 0.20187812 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# TODO: test the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")\n",
    "print(f\"Feature importance: {clf.feature_importances_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well.. this might mean that the inferences over the climb total and the length were quite accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict missing profiles\n",
    "missing_profiles = df[df['profile'].isna()]\n",
    "df.loc[missing_profiles.index, 'profile'] = clf.predict(missing_profiles[['length', 'climb_total', 'steepness', 'average_speed', 'is_ITT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_USE_COLS = [\n",
    "    # over time\n",
    "    'total_points',\n",
    "    'avg_points_per_race', \n",
    "    'average_position',\n",
    "    'avg_speed_cyclist', \n",
    "    'mean_stamina_index',\n",
    "    'race_count',\n",
    "    # race related\n",
    "    'length',\n",
    "    'climb_total', \n",
    "    'profile', \n",
    "    'startlist_quality', \n",
    "    'cyclist_age_rac', \n",
    "    'steepness', \n",
    "    'is_tarmac', \n",
    "    'stage_type',\n",
    "    # cyclist related\n",
    "    # 'height',\n",
    "    # 'weight',\n",
    "    'bmi',\n",
    "    'home_game',\n",
    "    # for the split\n",
    "    'date',\n",
    "    'target'\n",
    "]\n",
    "\n",
    "df_to_use = df[TO_USE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr, df_vl, df_ts = get_data_split(df_to_use)\n",
    "\n",
    "df_tr = df_tr.drop(columns=['date'])\n",
    "df_vl = df_vl.drop(columns=['date'])\n",
    "df_ts = df_ts.drop(columns=['date'])\n",
    "\n",
    "X_tr, y_tr = split_features_target(df_tr)\n",
    "X_vl, y_vl = split_features_target(df_vl)\n",
    "X_ts, y_ts = split_features_target(df_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the features before predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scal = StandardScaler()\n",
    "X_tr = scal.fit_transform(X_tr)\n",
    "X_vl = scal.transform(X_vl)\n",
    "X_ts = scal.transform(X_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_tr.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    # tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor (validation recall)\n",
    "    patience=6,            # Number of epochs with no improvement before stopping\n",
    "    mode='max',            # We want to maximize recall\n",
    "    restore_best_weights=True  # Restore the model weights from the epoch with the best recall\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[\n",
    "    'accuracy',\n",
    "    'recall'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5550/5550 - 9s - 2ms/step - accuracy: 0.8629 - loss: 0.3452 - recall: 0.1511 - val_accuracy: 0.8665 - val_loss: 0.3336 - val_recall: 0.2299\n",
      "Epoch 2/30\n",
      "5550/5550 - 9s - 2ms/step - accuracy: 0.8631 - loss: 0.3449 - recall: 0.1511 - val_accuracy: 0.8650 - val_loss: 0.3334 - val_recall: 0.2156\n",
      "Epoch 3/30\n",
      "5550/5550 - 8s - 1ms/step - accuracy: 0.8632 - loss: 0.3445 - recall: 0.1543 - val_accuracy: 0.8650 - val_loss: 0.3336 - val_recall: 0.1478\n",
      "Epoch 4/30\n",
      "5550/5550 - 8s - 1ms/step - accuracy: 0.8634 - loss: 0.3444 - recall: 0.1537 - val_accuracy: 0.8651 - val_loss: 0.3333 - val_recall: 0.1934\n",
      "Epoch 5/30\n",
      "5550/5550 - 8s - 1ms/step - accuracy: 0.8632 - loss: 0.3440 - recall: 0.1542 - val_accuracy: 0.8665 - val_loss: 0.3322 - val_recall: 0.1876\n",
      "Epoch 6/30\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_tr, y_tr, \n",
    "    validation_data=(X_vl, y_vl), \n",
    "    epochs=30, \n",
    "    batch_size=64,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1359/1359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.97      0.93     37243\n",
      "        True       0.58      0.22      0.32      6237\n",
      "\n",
      "    accuracy                           0.87     43480\n",
      "   macro avg       0.73      0.60      0.62     43480\n",
      "weighted avg       0.84      0.87      0.84     43480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "val_pred = model.predict(X_vl)\n",
    "val_pred = (val_pred > 0.5).astype(int)\n",
    "print(classification_report(y_vl, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449164    False\n",
      "449165     True\n",
      "449166    False\n",
      "449167    False\n",
      "449168    False\n",
      "          ...  \n",
      "492639    False\n",
      "492640    False\n",
      "492641    False\n",
      "492642    False\n",
      "492643    False\n",
      "Name: target, Length: 43480, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(y_vl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
