{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simpler models with the same results. Can this be considered a victory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 18:31:48.704683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735666308.818533   68460 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735666308.851408   68460 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-31 18:31:49.089376: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utility.classification_utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building our beautiful dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%  \n"
     ]
    }
   ],
   "source": [
    "cyc = '../dataset/cyclists_cleaned.csv'\n",
    "races = '../dataset/races_cleaned.csv'\n",
    "df = make_dataset_for_classification(races, cyc, make_stage_type=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumb imputations\n",
    "df['height'] = df['height'].fillna(df['height'].mean())\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "df['bmi'] = df['weight']/np.square(df['height']/100)\n",
    "df['cyclist_age_rac'] = df['cyclist_age_rac'].fillna(df['cyclist_age_rac'].mean())\n",
    "df['climb_total'] = df['climb_total'].fillna(df['length']*0.05) # could be differentiated by lengths\n",
    "df['steepness'] = df['length'] / df['climb_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for profile, let's use the big stick\n",
    "# Features and target\n",
    "# Add a column 'is_ITT' where 1 indicates stage_type is 'ITT', 0 otherwise\n",
    "df['is_ITT'] = (df['stage_type'] == 'ITT').astype(int)\n",
    "df_filtered = df[['length', 'climb_total', 'steepness', 'average_speed', 'is_ITT', 'profile']].dropna()\n",
    "X = df_filtered[['length', 'climb_total', 'steepness', 'average_speed', 'is_ITT']]\n",
    "y = df_filtered['profile']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# TODO: test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict missing profiles\n",
    "missing_profiles = df[df['profile'].isna()]\n",
    "df.loc[missing_profiles.index, 'profile'] = clf.predict(missing_profiles[['length', 'climb_total', 'steepness', 'average_speed', 'is_ITT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_USE_COLS = [\n",
    "    # over time\n",
    "    'total_points',\n",
    "    'avg_points_per_race', \n",
    "    'average_position',\n",
    "    'avg_speed_cyclist', \n",
    "    'mean_stamina_index',\n",
    "    'race_count',\n",
    "    # race related\n",
    "    'length',\n",
    "    'climb_total', \n",
    "    'profile', \n",
    "    'startlist_quality', \n",
    "    'cyclist_age_rac', \n",
    "    'steepness', \n",
    "    'is_tarmac', \n",
    "    'stage_type',\n",
    "    # cyclist related\n",
    "    'height',\n",
    "    'weight',\n",
    "    'bmi',\n",
    "    'home_game',\n",
    "    # for the split\n",
    "    'date',\n",
    "    'target'\n",
    "]\n",
    "\n",
    "df_to_use = df[TO_USE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr, df_vl, df_ts = get_data_split(df_to_use)\n",
    "\n",
    "df_tr = df_tr.drop(columns=['date'])\n",
    "df_vl = df_vl.drop(columns=['date'])\n",
    "df_ts = df_ts.drop(columns=['date'])\n",
    "\n",
    "X_tr, y_tr = split_features_target(df_tr)\n",
    "X_vl, y_vl = split_features_target(df_vl)\n",
    "X_ts, y_ts = split_features_target(df_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the features before predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scal = StandardScaler()\n",
    "X_tr = scal.fit_transform(X_tr)\n",
    "X_vl = scal.transform(X_vl)\n",
    "X_ts = scal.transform(X_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 18:38:29.770090: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_tr.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    # tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\n",
    "    'accuracy',\n",
    "    'recall',\n",
    "    'f1_score'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11099/11099 - 29s - 3ms/step - accuracy: 0.8569 - f1_score: 0.2540 - loss: 0.3598 - recall: 0.0964 - val_accuracy: 0.8643 - val_f1_score: 0.2509 - val_loss: 0.3383 - val_recall: 0.1289\n",
      "Epoch 2/5\n",
      "11099/11099 - 26s - 2ms/step - accuracy: 0.8595 - f1_score: 0.2540 - loss: 0.3535 - recall: 0.1110 - val_accuracy: 0.8633 - val_f1_score: 0.2509 - val_loss: 0.3361 - val_recall: 0.1550\n",
      "Epoch 3/5\n",
      "11099/11099 - 27s - 2ms/step - accuracy: 0.8598 - f1_score: 0.2540 - loss: 0.3524 - recall: 0.1150 - val_accuracy: 0.8648 - val_f1_score: 0.2509 - val_loss: 0.3357 - val_recall: 0.2113\n",
      "Epoch 4/5\n",
      "11099/11099 - 27s - 2ms/step - accuracy: 0.8599 - f1_score: 0.2540 - loss: 0.3517 - recall: 0.1181 - val_accuracy: 0.8644 - val_f1_score: 0.2509 - val_loss: 0.3340 - val_recall: 0.1975\n",
      "Epoch 5/5\n",
      "11099/11099 - 26s - 2ms/step - accuracy: 0.8602 - f1_score: 0.2540 - loss: 0.3509 - recall: 0.1228 - val_accuracy: 0.8644 - val_f1_score: 0.2509 - val_loss: 0.3339 - val_recall: 0.1621\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_tr, y_tr, \n",
    "    validation_data=(X_vl, y_vl), \n",
    "    epochs=5, \n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "val_pred = model.predict(X_vl)\n",
    "print(classification_report(y_vl, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449164    False\n",
      "449165     True\n",
      "449166    False\n",
      "449167    False\n",
      "449168    False\n",
      "          ...  \n",
      "492639    False\n",
      "492640    False\n",
      "492641    False\n",
      "492642    False\n",
      "492643    False\n",
      "Name: target, Length: 43480, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(y_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01081997]\n",
      " [0.56833375]\n",
      " [0.01587189]\n",
      " ...\n",
      " [0.17953698]\n",
      " [0.1327488 ]\n",
      " [0.07120073]]\n"
     ]
    }
   ],
   "source": [
    "print(val_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
